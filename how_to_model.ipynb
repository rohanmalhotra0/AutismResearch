{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How this model was built\n",
        "\n",
        "This notebook shows how we cleaned the autism screening data, engineered features, and trained a simple neural‑network style model (logistic regression in PyTorch) to estimate a probability.\n",
        "\n",
        "- Goal: Estimate ASD likelihood from A1–A10 items and demographics\n",
        "- Steps: Load → Clean/Encode → Split/Scale → Train (PyTorch) → Evaluate\n",
        "- Note: Educational only; not medical advice. Use responsibly and validate before any clinical use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports & reproducibility\n",
        "import os, random, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(f\"Using torch {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load, clean, encode\n",
        "CSV_PATH = 'autism_screening.csv'\n",
        "assert os.path.exists(CSV_PATH), f\"CSV not found at {CSV_PATH}\"\n",
        "\n",
        "def to_snake(s: str) -> str:\n",
        "    return (\n",
        "        s.strip().lower().replace('/', ' ').replace('-', ' ')\n",
        "        .replace('  ', ' ').replace(' ', '_')\n",
        "    )\n",
        "\n",
        "# 1) Load\n",
        "raw = pd.read_csv(CSV_PATH)\n",
        "raw.columns = [to_snake(c) for c in raw.columns]\n",
        "print(raw.shape, list(raw.columns)[:10])\n",
        "\n",
        "# 2) Basic normalization\n",
        "for c in raw.select_dtypes(include='object').columns:\n",
        "    raw[c] = raw[c].astype(str).str.strip().str.lower().replace({'?': np.nan, 'nan': np.nan})\n",
        "\n",
        "# Boolean mappings commonly used in this dataset\n",
        "bool_map = {'yes': 1, 'no': 0}\n",
        "for c in ['jundice','austim','used_app_before']:\n",
        "    if c in raw.columns:\n",
        "        raw[c] = raw[c].map(bool_map)\n",
        "\n",
        "if 'gender' in raw.columns:\n",
        "    raw['gender'] = raw['gender'].map({'m': 1, 'f': 0}).fillna(0)\n",
        "\n",
        "# A-scores (A1..A10) → ensure ints; also create result total\n",
        "as_cols = [f'a{i}_score' for i in range(1, 11) if f'a{i}_score' in raw.columns]\n",
        "for c in as_cols:\n",
        "    raw[c] = pd.to_numeric(raw[c], errors='coerce').fillna(0).astype(int)\n",
        "raw['result'] = raw[as_cols].sum(axis=1) if as_cols else 0\n",
        "\n",
        "# Target column detection\n",
        "TARGET = None\n",
        "for cand in ['class_asd', 'class','asd','label','target']:\n",
        "    if cand in raw.columns:\n",
        "        TARGET = cand\n",
        "        break\n",
        "assert TARGET is not None, 'Target column not found (expected class_asd or similar).'\n",
        "\n",
        "# Map target to 0/1\n",
        "if raw[TARGET].dtype == 'object':\n",
        "    raw[TARGET] = raw[TARGET].map(bool_map).fillna(0).astype(int)\n",
        "\n",
        "# One-hot encode selected categoricals; drop first to avoid collinearity\n",
        "cat_cols = [c for c in ['ethnicity','country_of_res','age_desc','relation'] if c in raw.columns]\n",
        "X = raw.drop(columns=[TARGET])\n",
        "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
        "\n",
        "# Impute numerics with median\n",
        "for c in X.columns:\n",
        "    if pd.api.types.is_numeric_dtype(X[c]):\n",
        "        X[c] = X[c].fillna(X[c].median())\n",
        "    else:\n",
        "        X[c] = X[c].fillna(0)\n",
        "\n",
        "y = raw[TARGET].astype(int)\n",
        "\n",
        "print('Features:', X.shape, 'Target:', y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split & scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "X_train_s = X_train_s.astype(np.float32)\n",
        "X_test_s = X_test_s.astype(np.float32)\n",
        "y_train_a = y_train.values.astype(np.float32).reshape(-1, 1)\n",
        "y_test_a = y_test.values.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "X_train_s.shape, X_test_s.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch logistic model & training\n",
        "class TorchLogReg(nn.Module):\n",
        "    def __init__(self, in_features: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, 1)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "in_features = X_train_s.shape[1]\n",
        "model = TorchLogReg(in_features)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train/val split from training set\n",
        "idx = np.arange(X_train_s.shape[0])\n",
        "np.random.shuffle(idx)\n",
        "val_ratio = 0.2\n",
        "val_n = int(len(idx) * val_ratio)\n",
        "val_idx, tr_idx = idx[:val_n], idx[val_n:]\n",
        "\n",
        "X_tr = torch.tensor(X_train_s[tr_idx], dtype=torch.float32).to(device)\n",
        "Y_tr = torch.tensor(y_train_a[tr_idx], dtype=torch.float32).to(device)\n",
        "X_val = torch.tensor(X_train_s[val_idx], dtype=torch.float32).to(device)\n",
        "Y_val = torch.tensor(y_train_a[val_idx], dtype=torch.float32).to(device)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_tr, Y_tr), batch_size=64, shuffle=True)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "best_val = float('inf')\n",
        "patience, bad_epochs = 5, 0\n",
        "history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item() * xb.size(0)\n",
        "    train_loss = running / len(train_loader.dataset)\n",
        "\n",
        "    # val\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = model(X_val)\n",
        "        val_loss = criterion(val_logits, Y_val).item()\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_loss + 1e-6 < best_val:\n",
        "        best_val = val_loss\n",
        "        bad_epochs = 0\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= patience:\n",
        "            break\n",
        "\n",
        "# load best\n",
        "model.load_state_dict(best_state)\n",
        "print('Best val loss:', best_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test & visualize training loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_te = torch.tensor(X_test_s, dtype=torch.float32).to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(X_te).cpu().numpy().ravel()\n",
        "    probs = 1 / (1 + np.exp(-logits))\n",
        "\n",
        "preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "acc = metrics.accuracy_score(y_test, preds)\n",
        "prec = metrics.precision_score(y_test, preds, zero_division=0)\n",
        "rec = metrics.recall_score(y_test, preds, zero_division=0)\n",
        "f1 = metrics.f1_score(y_test, preds, zero_division=0)\n",
        "roc = metrics.roc_auc_score(y_test, probs)\n",
        "print({'accuracy':acc, 'precision':prec, 'recall':rec, 'f1':f1, 'roc_auc':roc})\n",
        "\n",
        "# Training curve\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history['train_loss'], label='train')\n",
        "plt.plot(history['val_loss'], label='val')\n",
        "plt.title('Training/Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
