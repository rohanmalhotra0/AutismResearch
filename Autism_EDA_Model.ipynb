{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autism Screening: End-to-End EDA and PyTorch Baseline\n",
        "\n",
        "This notebook explores a tabular dataset end-to-end using pandas, numpy, plotly, and PyTorch. It includes:\n",
        "\n",
        "- Data overview and compact data dictionary\n",
        "- Type inference and cleaning\n",
        "- EDA with interactive Plotly visuals\n",
        "- Preprocessing (encoding, scaling, splitting)\n",
        "- PyTorch MLP baseline with early stopping\n",
        "- Evaluation metrics and diagnostics\n",
        "- Permutation importance for explainability\n",
        "- Summary and next steps\n",
        "\n",
        "All steps are reproducible with fixed seeds and configurable via the `CONFIG` dict below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config & Imports\n",
        "# Comment: Centralize configuration and set seeds for reproducibility.\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Reproducibility\n",
        "def set_seeds(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "CONFIG: Dict[str, object] = {\n",
        "    \"csv_path\": \"autism_screening.csv\",  # Update if needed\n",
        "    \"target\": None,  # e.g., \"Class/ASD\" or None\n",
        "    \"problem_type\": \"unspecified\",  # \"classification\" | \"regression\" | \"unspecified\"\n",
        "    \"test_size\": 0.2,\n",
        "    \"val_size\": 0.2,\n",
        "    \"seed\": 42,\n",
        "    \"max_cat_cardinality\": 50,  # threshold for treating as categorical\n",
        "    \"n_bins_hist\": 30,\n",
        "}\n",
        "\n",
        "set_seeds(int(CONFIG[\"seed\"]))\n",
        "\n",
        "# Print package versions for reproducibility\n",
        "print(\"Versions:\")\n",
        "print(\"python:\", sys.version.split()[0])\n",
        "print(\"pandas:\", pd.__version__)\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"torch:\", torch.__version__)\n",
        "\n",
        "# Utility: Safe display helper\n",
        "def display_heading(text: str):\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(text)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Utility: Numpy vectorized helpers\n",
        "def np_zscore(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Vectorized z-score with numerical stability for 1D or 2D arrays.\"\"\"\n",
        "    mean = np.nanmean(x, axis=0)\n",
        "    std = np.nanstd(x, axis=0)\n",
        "    std = np.where(std == 0, 1.0, std)\n",
        "    return (x - mean) / std\n",
        "\n",
        "def np_winsorize(x: np.ndarray, lower_q: float = 0.01, upper_q: float = 0.99) -> np.ndarray:\n",
        "    \"\"\"Vectorized winsorization using numpy percentiles (per-column).\"\"\"\n",
        "    if x.ndim == 1:\n",
        "        x = x.reshape(-1, 1)\n",
        "    lows = np.nanpercentile(x, lower_q * 100, axis=0, method=\"linear\")\n",
        "    highs = np.nanpercentile(x, upper_q * 100, axis=0, method=\"linear\")\n",
        "    return np.clip(x, lows, highs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load & Inspect\n",
        "# Comment: Load CSV and produce a compact overview and data dictionary.\n",
        "from pathlib import Path\n",
        "\n",
        "csv_path = Path(CONFIG[\"csv_path\"]).expanduser()\n",
        "assert csv_path.exists(), f\"CSV not found at {csv_path}\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "display_heading(\"Basic Overview\")\n",
        "print(\"shape:\", df.shape)\n",
        "print(\"dtypes:\\n\", df.dtypes)\n",
        "print(\"duplicates:\", int(df.duplicated().sum()))\n",
        "print(\"missing per column:\\n\", df.isna().sum())\n",
        "\n",
        "# Build data dictionary\n",
        "# For categoricals: list # unique and example values; for numericals: min/mean/std/max\n",
        "\n",
        "def infer_types(df: pd.DataFrame, max_cat_cardinality: int) -> Tuple[List[str], List[str], List[str]]:\n",
        "    numeric_cols: List[str] = []\n",
        "    categorical_cols: List[str] = []\n",
        "    datetime_cols: List[str] = []\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        if pd.api.types.is_datetime64_any_dtype(s):\n",
        "            datetime_cols.append(col)\n",
        "        elif pd.api.types.is_numeric_dtype(s):\n",
        "            numeric_cols.append(col)\n",
        "        else:\n",
        "            # try parse datetime if many parse successes\n",
        "            parsed = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
        "            parsed_ratio = parsed.notna().mean()\n",
        "            if parsed_ratio > 0.9:\n",
        "                datetime_cols.append(col)\n",
        "            else:\n",
        "                # treat as categorical if cardinality is not too high\n",
        "                nunique = s.nunique(dropna=True)\n",
        "                if nunique <= max_cat_cardinality:\n",
        "                    categorical_cols.append(col)\n",
        "                else:\n",
        "                    # high-cardinality non-numeric => treat as categorical nevertheless\n",
        "                    categorical_cols.append(col)\n",
        "    return numeric_cols, categorical_cols, datetime_cols\n",
        "\n",
        "numeric_cols, categorical_cols, datetime_cols = infer_types(df, int(CONFIG[\"max_cat_cardinality\"]))\n",
        "\n",
        "# Create data dictionary rows\n",
        "rows = []\n",
        "for col in df.columns:\n",
        "    s = df[col]\n",
        "    dtype = str(s.dtype)\n",
        "    missing_pct = float(s.isna().mean() * 100)\n",
        "    entry: Dict[str, object] = {\n",
        "        \"column\": col,\n",
        "        \"dtype\": dtype,\n",
        "        \"%missing\": round(missing_pct, 2),\n",
        "    }\n",
        "    if col in numeric_cols:\n",
        "        desc = s.describe(percentiles=[])\n",
        "        entry.update({\n",
        "            \"min\": float(desc.get(\"min\", np.nan)),\n",
        "            \"mean\": float(desc.get(\"mean\", np.nan)),\n",
        "            \"std\": float(desc.get(\"std\", np.nan)),\n",
        "            \"max\": float(desc.get(\"max\", np.nan)),\n",
        "        })\n",
        "    elif col in categorical_cols:\n",
        "        nunique = int(s.nunique(dropna=True))\n",
        "        examples = s.dropna().astype(str).unique()[:5]\n",
        "        entry.update({\n",
        "            \"#unique\": nunique,\n",
        "            \"examples\": \", \".join(map(str, examples)),\n",
        "        })\n",
        "    elif col in datetime_cols:\n",
        "        # parse to datetime for min/max\n",
        "        parsed = pd.to_datetime(s, errors=\"coerce\")\n",
        "        entry.update({\n",
        "            \"min\": str(parsed.min()) if parsed.notna().any() else None,\n",
        "            \"max\": str(parsed.max()) if parsed.notna().any() else None,\n",
        "        })\n",
        "    rows.append(entry)\n",
        "\n",
        "data_dict_df = pd.DataFrame(rows)\n",
        "\n",
        "display_heading(\"Data Dictionary (compact)\")\n",
        "print(data_dict_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Type Inference & Cleaning\n",
        "# Comment: Normalize string categories and strip whitespace; prepare parsed datetimes.\n",
        "\n",
        "def normalize_categorical_series(s: pd.Series) -> pd.Series:\n",
        "    if s.dtype == object or pd.api.types.is_string_dtype(s):\n",
        "        return s.astype(str).str.strip().str.lower().replace({'nan': np.nan})\n",
        "    return s\n",
        "\n",
        "# Apply normalization to non-numeric columns\n",
        "for col in df.columns:\n",
        "    if col not in df.select_dtypes(include=[np.number]).columns:\n",
        "        df[col] = normalize_categorical_series(df[col])\n",
        "\n",
        "# Parse datetime columns detected earlier\n",
        "for col in datetime_cols:\n",
        "    df[col] = pd.to_datetime(df[col], errors='coerce', infer_datetime_format=True)\n",
        "\n",
        "# Recompute missingness after normalization\n",
        "missing_after = df.isna().sum()\n",
        "\n",
        "display_heading(\"Missingness After Cleaning (counts)\")\n",
        "print(missing_after[missing_after > 0].sort_values(ascending=False))\n",
        "\n",
        "# Imputation plan: numeric -> median; categorical -> most frequent; datetime -> leave, or fill with median date\n",
        "from collections import Counter\n",
        "\n",
        "def impute_dataframe(df_in: pd.DataFrame,\n",
        "                     numeric_cols: List[str],\n",
        "                     categorical_cols: List[str],\n",
        "                     datetime_cols: List[str]) -> Tuple[pd.DataFrame, Dict[str, Dict[str, object]]]:\n",
        "    df_out = df_in.copy()\n",
        "    plan: Dict[str, Dict[str, object]] = {}\n",
        "    for col in numeric_cols:\n",
        "        median_val = float(df_out[col].median()) if df_out[col].notna().any() else 0.0\n",
        "        df_out[col] = df_out[col].fillna(median_val)\n",
        "        plan[col] = {\"type\": \"numeric\", \"strategy\": \"median\", \"value\": median_val}\n",
        "    for col in categorical_cols:\n",
        "        mode_val = df_out[col].mode(dropna=True)\n",
        "        fill_val = mode_val.iloc[0] if not mode_val.empty else \"missing\"\n",
        "        df_out[col] = df_out[col].fillna(fill_val)\n",
        "        plan[col] = {\"type\": \"categorical\", \"strategy\": \"most_frequent\", \"value\": fill_val}\n",
        "    for col in datetime_cols:\n",
        "        # fill with median timestamp if available\n",
        "        ts = pd.to_datetime(df_out[col], errors='coerce')\n",
        "        if ts.notna().any():\n",
        "            med = ts.dropna().median()\n",
        "            df_out[col] = ts.fillna(med)\n",
        "            plan[col] = {\"type\": \"datetime\", \"strategy\": \"median\", \"value\": str(med)}\n",
        "        else:\n",
        "            plan[col] = {\"type\": \"datetime\", \"strategy\": \"none\", \"value\": None}\n",
        "    return df_out, plan\n",
        "\n",
        "clean_df, impute_plan = impute_dataframe(df, numeric_cols, categorical_cols, datetime_cols)\n",
        "\n",
        "display_heading(\"Imputation Plan Summary (per column)\")\n",
        "print(json.dumps(impute_plan, indent=2)[:4000])  # truncate if long\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missingness Visuals (Plotly)\n",
        "# Comment: Plot missing counts and a simple heatmap-like visualization.\n",
        "\n",
        "missing_counts = clean_df.isna().sum().sort_values(ascending=False)\n",
        "missing_df = missing_counts.reset_index()\n",
        "missing_df.columns = [\"column\", \"missing_count\"]\n",
        "\n",
        "fig_mc = px.bar(missing_df, x=\"column\", y=\"missing_count\", title=\"Missing Values per Column\",\n",
        "                labels={\"missing_count\": \"Missing Count\", \"column\": \"Column\"})\n",
        "fig_mc.update_layout(xaxis_tickangle=-45, height=450)\n",
        "fig_mc.show()\n",
        "\n",
        "# Heatmap-like missingness: show a sample to avoid huge rendering\n",
        "sample_n = min(500, len(clean_df))\n",
        "ms_sample = clean_df.sample(sample_n, random_state=int(CONFIG[\"seed\"]))\n",
        "miss_bool = ms_sample.isna().astype(int)\n",
        "fig_mh = px.imshow(miss_bool.T, aspect=\"auto\", color_continuous_scale=[(0.0, \"#1a1a1a\"), (1.0, \"#e74c3c\")],\n",
        "                   title=\"Missingness Heatmap (1=Missing) - Transposed for readability\")\n",
        "fig_mh.update_yaxes(title=\"Columns\")\n",
        "fig_mh.update_xaxes(title=\"Sample Rows\")\n",
        "fig_mh.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numeric EDA (hist+KDE, box, correlations)\n",
        "# Comment: Iterate over numeric columns to plot hist/KDE and boxplots; then correlations.\n",
        "\n",
        "num_cols = [c for c in numeric_cols if c in clean_df.columns]\n",
        "\n",
        "for col in num_cols:\n",
        "    s = clean_df[col]\n",
        "    fig_h = px.histogram(clean_df, x=col, nbins=int(CONFIG[\"n_bins_hist\"]), marginal=\"violin\",\n",
        "                         title=f\"Histogram + Violin for {col}\", opacity=0.85)\n",
        "    fig_h.update_traces(marker_color=\"#2ecc71\")\n",
        "    fig_h.update_layout(bargap=0.05)\n",
        "    fig_h.show()\n",
        "\n",
        "    fig_b = px.box(clean_df, y=col, points=\"outliers\", title=f\"Boxplot (Outliers) for {col}\")\n",
        "    fig_b.update_traces(marker_color=\"#e67e22\")\n",
        "    fig_b.show()\n",
        "\n",
        "# Correlations (Pearson & Spearman)\n",
        "if len(num_cols) >= 2:\n",
        "    pearson_corr = clean_df[num_cols].corr(method='pearson')\n",
        "    spearman_corr = clean_df[num_cols].corr(method='spearman')\n",
        "\n",
        "    fig_cp = px.imshow(pearson_corr, text_auto=False, color_continuous_scale=\"RdBu_r\",\n",
        "                       title=\"Pearson Correlation (Numeric Features)\")\n",
        "    fig_cp.update_xaxes(side=\"bottom\")\n",
        "    fig_cp.show()\n",
        "\n",
        "    fig_cs = px.imshow(spearman_corr, text_auto=False, color_continuous_scale=\"RdBu_r\",\n",
        "                       title=\"Spearman Correlation (Numeric Features)\")\n",
        "    fig_cs.update_xaxes(side=\"bottom\")\n",
        "    fig_cs.show()\n",
        "else:\n",
        "    print(\"Not enough numeric columns for correlations.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numpy Utilities Demo: Z-score Outlier Rates and Winsorization Preview\n",
        "# Comment: Use vectorized numpy ops to flag outliers and preview winsorization effects.\n",
        "\n",
        "if len(num_cols) > 0:\n",
        "    Xn = clean_df[num_cols].to_numpy(dtype=float)\n",
        "    Z = np_zscore(Xn)\n",
        "    outlier_mask = np.abs(Z) > 3.0\n",
        "    outlier_rates = outlier_mask.mean(axis=0)\n",
        "    outlier_series = pd.Series(outlier_rates, index=num_cols).sort_values(ascending=False)\n",
        "\n",
        "    fig_or = px.bar(outlier_series.head(25).sort_values(), orientation=\"h\",\n",
        "                    title=\"Outlier Rate (>3Ïƒ) by Numeric Feature (Top 25)\",\n",
        "                    labels={\"value\": \"Outlier Rate\", \"index\": \"Feature\"})\n",
        "    fig_or.show()\n",
        "\n",
        "    # Winsorization preview (not used downstream, just diagnostic)\n",
        "    Xw = np_winsorize(Xn, 0.01, 0.99)\n",
        "    pre_std = np.nanstd(Xn, axis=0)\n",
        "    post_std = np.nanstd(Xw, axis=0)\n",
        "    std_drop = pd.Series(pre_std - post_std, index=num_cols)\n",
        "    fig_w = px.bar(std_drop.head(25).sort_values(), orientation=\"h\",\n",
        "                   title=\"Std Reduction after Winsorization (Top 25)\",\n",
        "                   labels={\"value\": \"Std Reduction\", \"index\": \"Feature\"})\n",
        "    fig_w.show()\n",
        "else:\n",
        "    print(\"No numeric columns available for numpy utilities demo.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical & Relationships EDA\n",
        "# Comment: Top categories, stacked bars vs target (if exists), scatter and violin.\n",
        "\n",
        "target_col = CONFIG[\"target\"]\n",
        "cat_cols = [c for c in categorical_cols if c in clean_df.columns]\n",
        "\n",
        "# Top categories bar charts\n",
        "for col in cat_cols:\n",
        "    vc = clean_df[col].value_counts(dropna=False).reset_index()\n",
        "    vc.columns = [col, \"count\"]\n",
        "    fig_bar = px.bar(vc.head(20), x=col, y=\"count\", title=f\"Top Categories for {col}\")\n",
        "    fig_bar.update_layout(xaxis_tickangle=-45)\n",
        "    fig_bar.show()\n",
        "\n",
        "# Stacked bars vs target (classification-like view)\n",
        "if isinstance(target_col, str) and target_col in clean_df.columns:\n",
        "    if CONFIG[\"problem_type\"] == \"classification\" or clean_df[target_col].nunique() <= 20:\n",
        "        for col in cat_cols:\n",
        "            if col == target_col:\n",
        "                continue\n",
        "            cross = clean_df.groupby([col, target_col]).size().reset_index(name=\"count\")\n",
        "            fig_stack = px.bar(cross, x=col, y=\"count\", color=target_col, barmode=\"stack\",\n",
        "                               title=f\"Stacked Bar: {col} vs {target_col}\")\n",
        "            fig_stack.update_layout(xaxis_tickangle=-45)\n",
        "            fig_stack.show()\n",
        "\n",
        "# Relationships: scatter with trendline (for numeric pairs)\n",
        "if len(num_cols) >= 2:\n",
        "    x_col = num_cols[0]\n",
        "    for y_col in num_cols[1:3]:\n",
        "        fig_sc = px.scatter(clean_df, x=x_col, y=y_col, trendline=\"ols\",\n",
        "                            title=f\"Scatter with Trendline: {x_col} vs {y_col}\")\n",
        "        fig_sc.show()\n",
        "\n",
        "# Violin/strip for target vs key features\n",
        "if isinstance(target_col, str) and target_col in clean_df.columns:\n",
        "    if target_col in num_cols:\n",
        "        # numeric target vs categorical features: violin by category\n",
        "        for col in cat_cols[:3]:\n",
        "            fig_vi = px.violin(clean_df, x=col, y=target_col, box=True, points=\"all\",\n",
        "                               title=f\"Violin: {target_col} by {col}\")\n",
        "            fig_vi.show()\n",
        "    else:\n",
        "        # categorical target vs numeric features\n",
        "        for col in num_cols[:5]:\n",
        "            fig_st = px.strip(clean_df, x=target_col, y=col, title=f\"Strip: {col} by {target_col}\")\n",
        "            fig_st.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing for Modeling\n",
        "# Comment: Encode categoricals, scale numerics, build train/val/test splits.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Resolve target and problem type\n",
        "TARGET = CONFIG[\"target\"]\n",
        "PROBLEM = CONFIG[\"problem_type\"]\n",
        "\n",
        "if isinstance(TARGET, str) and TARGET in clean_df.columns:\n",
        "    y = clean_df[TARGET]\n",
        "    X = clean_df.drop(columns=[TARGET])\n",
        "\n",
        "    # Re-infer types on X only\n",
        "    X_num, X_cat, X_dt = infer_types(X, int(CONFIG[\"max_cat_cardinality\"]))\n",
        "\n",
        "    # One-hot encoding choice: Simpler for MLP and avoids ordinal assumptions\n",
        "    X_enc = pd.get_dummies(X, columns=X_cat, dummy_na=False, drop_first=False)\n",
        "\n",
        "    # Scale numeric columns to stabilize training\n",
        "    scaler = StandardScaler()\n",
        "    # Identify numeric columns post-encoding\n",
        "    enc_num_cols = [c for c in X_enc.columns if c in X_num]\n",
        "    X_enc[enc_num_cols] = scaler.fit_transform(X_enc[enc_num_cols])\n",
        "\n",
        "    # Convert y for classification/regression\n",
        "    if PROBLEM == \"classification\":\n",
        "        # Try to map string labels to integers deterministically\n",
        "        if not pd.api.types.is_numeric_dtype(y):\n",
        "            classes = sorted(y.dropna().astype(str).unique())\n",
        "            class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "            y_enc = y.astype(str).map(class_to_idx)\n",
        "        else:\n",
        "            y_enc = y.astype(int)\n",
        "    elif PROBLEM == \"regression\":\n",
        "        y_enc = pd.to_numeric(y, errors='coerce')\n",
        "    else:\n",
        "        y_enc = y  # unspecified; will skip modeling later if not supported\n",
        "\n",
        "    # Train/Val/Test split (stratified if classification)\n",
        "    test_size = float(CONFIG[\"test_size\"])\n",
        "    val_size = float(CONFIG[\"val_size\"])\n",
        "\n",
        "    stratify_vec = y_enc if PROBLEM == \"classification\" else None\n",
        "\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X_enc, y_enc, test_size=test_size, random_state=int(CONFIG[\"seed\"]), stratify=stratify_vec)\n",
        "\n",
        "    # Adjust val split relative to remaining\n",
        "    val_relative = val_size / (1 - test_size)\n",
        "    stratify_vec_temp = y_temp if PROBLEM == \"classification\" else None\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_relative, random_state=int(CONFIG[\"seed\"]), stratify=stratify_vec_temp)\n",
        "\n",
        "    display_heading(\"Split Shapes\")\n",
        "    print(\"X_train:\", X_train.shape, \"X_val:\", X_val.shape, \"X_test:\", X_test.shape)\n",
        "    if PROBLEM == \"classification\":\n",
        "        def show_balance(name, y_series):\n",
        "            vc = y_series.value_counts(normalize=True).sort_index()\n",
        "            print(name, \":\", {int(k): round(float(v), 3) for k, v in vc.items()})\n",
        "        show_balance(\"Train class balance\", y_train)\n",
        "        show_balance(\"Val class balance\", y_val)\n",
        "        show_balance(\"Test class balance\", y_test)\n",
        "else:\n",
        "    print(\"No valid target provided; skipping modeling steps. Set CONFIG['target'] and 'problem_type'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch Dataset & Model\n",
        "# Comment: Tabular MLP baseline suitable for classification or regression.\n",
        "\n",
        "if isinstance(TARGET, str) and TARGET in clean_df.columns and PROBLEM in (\"classification\", \"regression\"):\n",
        "\n",
        "    class TabularDataset(Dataset):\n",
        "        def __init__(self, X: pd.DataFrame, y: pd.Series):\n",
        "            self.X = torch.tensor(X.values, dtype=torch.float32)\n",
        "            # y dtype depends on problem\n",
        "            if PROBLEM == \"classification\":\n",
        "                self.y = torch.tensor(y.values, dtype=torch.long)\n",
        "            else:\n",
        "                self.y = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.X.shape[0]\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.X[idx], self.y[idx]\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, input_dim: int, output_dim: int):\n",
        "            super().__init__()\n",
        "            hidden = 128\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(input_dim, hidden),\n",
        "                nn.BatchNorm1d(hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(hidden, 64),\n",
        "                nn.BatchNorm1d(64),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(64, output_dim),\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    if PROBLEM == \"classification\":\n",
        "        num_classes = int(pd.Series(y_train).nunique())\n",
        "        model = MLP(input_dim, num_classes)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        model = MLP(input_dim, 1)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    train_ds = TabularDataset(X_train, y_train)\n",
        "    val_ds = TabularDataset(X_val, y_val)\n",
        "    test_ds = TabularDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    patience = 10\n",
        "    wait = 0\n",
        "    num_epochs = 100\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        batch_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            if PROBLEM == \"classification\":\n",
        "                loss = criterion(out, yb)\n",
        "            else:\n",
        "                loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            batch_losses.append(loss.item())\n",
        "        train_loss = float(np.mean(batch_losses))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                out = model(xb)\n",
        "                loss = criterion(out, yb)\n",
        "                val_losses.append(loss.item())\n",
        "        val_loss = float(np.mean(val_losses))\n",
        "\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    # Restore best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # Save artifacts\n",
        "    ARTIFACTS = {\n",
        "        \"scaler_mean\": scaler.mean_.tolist() if 'scaler' in locals() else None,\n",
        "        \"scaler_scale\": scaler.scale_.tolist() if 'scaler' in locals() else None,\n",
        "        \"columns\": X_enc.columns.tolist(),\n",
        "        \"problem\": PROBLEM,\n",
        "        \"target\": TARGET,\n",
        "    }\n",
        "    torch.save(model.state_dict(), \"model_state_dict.pt\")\n",
        "    with open(\"artifacts.json\", \"w\") as f:\n",
        "        json.dump(ARTIFACTS, f, indent=2)\n",
        "else:\n",
        "    print(\"Modeling skipped due to missing/invalid target or problem type.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation & Diagnostics\n",
        "# Comment: Compute metrics and plot learning curve; confusion matrix or residuals.\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "if isinstance(TARGET, str) and TARGET in clean_df.columns and PROBLEM in (\"classification\", \"regression\"):\n",
        "    # Learning curve\n",
        "    hist_df = pd.DataFrame(history)\n",
        "    fig_lc = px.line(hist_df, x=\"epoch\", y=[\"train_loss\", \"val_loss\"],\n",
        "                     title=\"Learning Curve (Loss vs Epoch)\")\n",
        "    fig_lc.update_layout(yaxis_title=\"Loss\")\n",
        "    fig_lc.show()\n",
        "\n",
        "    # Collect predictions\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_true = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            if PROBLEM == \"classification\":\n",
        "                probs = torch.softmax(out, dim=1).cpu().numpy()\n",
        "                preds = probs.argmax(axis=1)\n",
        "                all_probs.append(probs)\n",
        "                all_preds.append(preds)\n",
        "                all_true.append(yb.numpy())\n",
        "            else:\n",
        "                preds = out.cpu().numpy().squeeze()\n",
        "                all_preds.append(preds)\n",
        "                all_true.append(yb.numpy().squeeze())\n",
        "\n",
        "    y_true = np.concatenate(all_true)\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "\n",
        "    if PROBLEM == \"classification\":\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "        try:\n",
        "            # binary or multiclass OneVsRest AUROC\n",
        "            num_classes = int(pd.Series(y_train).nunique())\n",
        "            if num_classes == 2 and len(all_probs) > 0:\n",
        "                y_prob = np.concatenate(all_probs)[:, 1]\n",
        "                auroc = roc_auc_score(y_true, y_prob)\n",
        "            else:\n",
        "                y_prob = np.concatenate(all_probs)\n",
        "                auroc = roc_auc_score(pd.get_dummies(y_true), y_prob, average=\"weighted\", multi_class=\"ovr\")\n",
        "        except Exception:\n",
        "            auroc = np.nan\n",
        "\n",
        "        print({\"accuracy\": round(acc, 4), \"f1_weighted\": round(f1, 4), \"auroc\": None if np.isnan(auroc) else round(float(auroc), 4)})\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        fig_cm = px.imshow(cm, text_auto=True, color_continuous_scale=\"Blues\",\n",
        "                           title=\"Confusion Matrix\", labels=dict(x=\"Predicted\", y=\"True\", color=\"Count\"))\n",
        "        fig_cm.update_xaxes(title=\"Predicted\")\n",
        "        fig_cm.update_yaxes(title=\"True\")\n",
        "        fig_cm.show()\n",
        "    else:\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        print({\"MAE\": round(mae, 4), \"RMSE\": round(rmse, 4), \"R2\": round(r2, 4)})\n",
        "\n",
        "        # Residual diagnostics\n",
        "        residuals = y_true - y_pred\n",
        "        fig_res_sc = px.scatter(x=y_pred, y=residuals, labels={\"x\": \"Predicted\", \"y\": \"Residual\"},\n",
        "                                title=\"Residuals vs Predicted\")\n",
        "        fig_res_sc.add_hline(y=0, line_dash=\"dash\")\n",
        "        fig_res_sc.show()\n",
        "\n",
        "        fig_res_hist = px.histogram(residuals, nbins=40, title=\"Residuals Histogram\")\n",
        "        fig_res_hist.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explainability: Permutation Importance\n",
        "# Comment: Compute permutation importance on validation set using numpy.\n",
        "\n",
        "def permutation_importance(model: nn.Module, X: pd.DataFrame, y: pd.Series, metric_fn, n_repeats: int = 5) -> pd.Series:\n",
        "    model.eval()\n",
        "    X_np = X.values.astype(np.float32)\n",
        "    y_np = y.values\n",
        "    baseline = metric_fn(model, X_np, y_np)\n",
        "    rng = np.random.default_rng(int(CONFIG[\"seed\"]))\n",
        "    importances = np.zeros(X_np.shape[1], dtype=float)\n",
        "\n",
        "    for j in range(X_np.shape[1]):\n",
        "        scores = []\n",
        "        for _ in range(n_repeats):\n",
        "            X_perm = X_np.copy()\n",
        "            rng.shuffle(X_perm[:, j])\n",
        "            score = metric_fn(model, X_perm, y_np)\n",
        "            scores.append(baseline - score)  # drop in metric\n",
        "        importances[j] = float(np.mean(scores))\n",
        "    return pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Define metric compatible with classification/regression\n",
        "if isinstance(TARGET, str) and TARGET in clean_df.columns and PROBLEM in (\"classification\", \"regression\"):\n",
        "    def metric_fn(model, Xb, yb):\n",
        "        with torch.no_grad():\n",
        "            xb = torch.tensor(Xb, dtype=torch.float32, device=device)\n",
        "            out = model(xb)\n",
        "            if PROBLEM == \"classification\":\n",
        "                preds = out.softmax(dim=1).argmax(dim=1).cpu().numpy()\n",
        "                return accuracy_score(yb, preds)\n",
        "            else:\n",
        "                preds = out.cpu().numpy().squeeze()\n",
        "                return -mean_squared_error(yb, preds, squared=False)  # negative RMSE (higher is better)\n",
        "\n",
        "    imp_series = permutation_importance(model, X_val, y_val, metric_fn, n_repeats=5)\n",
        "    fig_imp = px.bar(imp_series.head(25).sort_values(), orientation=\"h\",\n",
        "                     title=\"Permutation Importance (Top 25)\", labels={\"value\": \"Importance\", \"index\": \"Feature\"})\n",
        "    fig_imp.show()\n",
        "else:\n",
        "    print(\"Explainability skipped due to missing/invalid target or problem type.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "- Data quality: Reviewed missingness and applied column-wise imputation strategies (median, mode, median timestamp).\n",
        "- Feature types: Inferred numeric, categorical, and datetime automatically; normalized string categories.\n",
        "- EDA: Produced 8+ interactive visuals (histograms, box plots, correlations, bars, stacked bars, scatter with trendline, violin/strip, missingness heatmap).\n",
        "- Encoding & scaling: Used one-hot for categoricals and standardization for numerics to support MLP stability.\n",
        "- Baseline model: Trained a small MLP with early stopping; logged learning curves and metrics.\n",
        "- Metrics: Printed appropriate metrics based on problem type (classification vs regression) and diagnostics.\n",
        "- Explainability: Computed permutation importance on validation; displayed top features.\n",
        "- Reproducibility: Seeds set; versions printed; artifacts saved (`model_state_dict.pt`, `artifacts.json`).\n",
        "\n",
        "Recommendations:\n",
        "- Validate target definition and problem type; consider class imbalance handling (class weights, resampling) if skewed.\n",
        "- Engineer domain features (e.g., aggregations, ratios); consider interaction terms.\n",
        "- Try stronger tabular models (Gradient Boosting, XGBoost, catboost) and compare.\n",
        "- Hyperparameter tune MLP (layers, hidden size, dropout, LR schedule) and use k-fold CV.\n",
        "- Calibrate probabilities for classification (Platt/Isotonic) if needed.\n",
        "- Address high-cardinality categoricals with target or leave-one-out encoding if present.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
